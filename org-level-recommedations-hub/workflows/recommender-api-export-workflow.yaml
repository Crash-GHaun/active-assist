# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# TODO: Scope down from these large permissions.
# Required Permissions:
# Bigquery Admin  on the BigQuery Project
# serviceusage.serviceUsageAdmin (To enable services)
# Recommenders Exporter at the org level
# Cloud Asset Viewer at the org level

main:
  # datasetId = the dataset in bigquery you want the export to take place to. must be in format: projects/projectId/datasets/datasetId
  # ^ This might not be true, but gonna do some test. However datasets must be underscore not dashed. 
  # assetTable = the table in bigquery you want to export data to.
  # levelToExport = The Org/Project/Folder level you want exports to. I.E organizations/123 or projects/my-project-id or folders/123
  # Sadly this ^ one won't be available soon as it doesn't look like the recommender API does exports at project level unless you call it directly.
  # bqLocation = location of BQ you want to use.
  # orgId = The organization required for export. 
  # projectId
  params: [args]
  steps:
    - checkInput:
        switch:
          - condition: ${not("datasetId" in args)}
            raise: "Arg datasetId not specified."
          - condition: ${not("assetTable" in args)}
            raise: "Arg assetTable not specified."
          - condition: ${not("recommendationTable" in args)}
            raise: "Arg recommendationTable not specified"
          - condition: ${not("bqLocation" in args)}
            raise: "Arg bqLocation not specified"
          - condition: ${not("levelToExport" in args)}
            raise: "Arg levelToExport not specified."
          - condition: ${not("projectId" in args)}
            raise: "Arg projectId not specified."
          - condition: ${not("orgId" in args)}
            raise: "Arg OrgId not specified."
    - init:
        assign:
        - datasetId: ${args.datasetId}
        - assetTable: ${args.assetTable}
        - bqLocation: ${args.bqLocation}
        - levelToExport: ${args.levelToExport}
        - projectId: ${args.projectId}
        - orgId: ${args.orgId}
        - recommendationStatesTableId: ${args.recommendationTable}
    - enableBigQueryApiIfNeeded:
        call: checkAndEnableApiService
        args:
          projectId: ${projectId}
          API: "bigquerydatatransfer.googleapis.com"
    - enableCloudAssetApiIfNeeded:
        call: checkAndEnableApiService
        args:
          projectId: ${projectId}
          API: "cloudasset.googleapis.com"
    - createDatasetIfNeeded:
        call: create_dataset_if_needed
        args:
          projectId: ${projectId}
          datasetId: ${datasetId}
          region: ${bqLocation}
    - enrollDataSource:
        call: enrollDataSourceIfNeeded
        args:
          projectId: ${projectId}
    - createDataExport:
        call: createDataExportIfNeeded
        args: 
          projectId: ${projectId}
          orgId: ${orgId}
          datasetId: ${datasetId}
        result: configId
    - runExports:
        call: runExportsInParallel
        args:
          bqDataset: ${datasetId}
          bqTable: ${assetTable}
          projectId: ${projectId}
          orgId: ${orgId}
          levelToExport: ${levelToExport}
          transferId: ${configId}
    - returnOutput:
        return: "Finished"

# A Function to check if an API is enabled, and if not enable it. 
checkAndEnableApiService:
  params: [projectId, API]
  steps:
  - checkIfServiceEnabled:
        call: http.get
        args:
          url: ${"https://serviceusage.googleapis.com/v1/projects/"+ projectId + "/services/" + API}
          auth:
            type: OAuth2
            scopes:
              - "https://www.googleapis.com/auth/cloud-platform.read-only"
        result: apiResponse
  # The problem with enabling the API is it can take some time to propagate.
  - checkApiResponse:
      switch:
      #enable the API if not enabled
        - condition: ${apiResponse.body.state != "ENABLED"}
          steps:
            - enableAPI:
                call: http.post
                args:
                  url: ${"https://serviceusage.googleapis.com/v1/projects/"+ projectId + "/services/" + API +":enable"}
                  body: 
                    # body is empty according to:
                    # https://cloud.google.com/service-usage/docs/reference/rest/v1/services/enable
                  auth:
                    type: OAuth2
                    scopes: 
                      - "https://www.googleapis.com/auth/cloud-platform"
                      - "https://www.googleapis.com/auth/service.management"
            - waitForAPIPropagation:
                call: sys.sleep
                args:
                  seconds: 60
        

# TODO (Ghaun): Updating naming convention instead of having multiple styles
# Need to lookup best practices for yaml
create_dataset_if_needed:
    params: [projectId, datasetId, region]
    steps:
    - checkIfDatasetExists:
        try:
            call: googleapis.bigquery.v2.datasets.get
            args:
                datasetId: ${datasetId}
                projectId: ${projectId}
            result: getResult
        except:
            as: e
            steps:
                - known_errors:
                    switch:
                    # Create the dataset if it doesn't exist.
                        - condition: ${e.code == 404}
                          steps:
                            - createDataset:
                                call: googleapis.bigquery.v2.datasets.insert
                                args:
                                    projectId: ${projectId}
                                    body:
                                        datasetReference:
                                            datasetId: ${datasetId}
                                            projectId: ${projectId}
                                        access[].role: "roles/bigquery.dataViewer"
                                        access[].specialGroup: "projectReaders"
                                        location: ${region}
                        - condition: ${e.code != 404}
                          steps:
                            - raiseError:
                                raise: ${e}

# This enrolls the recommenders API to the project so BQ can transfer from it.
# For official documentation visit:
# https://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq#set_up_the_export_using_bigquery_command_line_rest_api 
enrollDataSourceIfNeeded:
  params: [projectId]
  steps:
  - checkIfDatasourceExists:
      try:
        call: googleapis.bigquerydatatransfer.v1.projects.dataSources.get
        args:
          name: ${"projects/" + projectId + "/dataSources/6063d10f-0000-2c12-a706-f403045e6250"}
        result: getResults
      except:
        as: e
        steps:
          - known_errors:
              switch:
                # Add the Datasource if it doesn't exist.
                  - condition: ${e.code == 400}
                    steps:
                      # there is a solid chance this will fail because the connector doesn't have this method listed
                      # https://cloud.google.com/workflows/docs/reference/googleapis/bigquerydatatransfer/v1/projects.locations.dataSources/get
                      - enrollInDatasource:
                          call: http.post
                          args:
                            url: ${"https://bigquerydatatransfer.googleapis.com/v1/projects/"+ projectId + ":enrollDataSources"}
                            body:
                              dataSourceIds:
                                - "6063d10f-0000-2c12-a706-f403045e6250"
                            auth:
                              type: OAuth2
                              scopes: "https://www.googleapis.com/auth/cloud-platform"
                  - condition: ${e.code != 400}
                    steps:
                      - raiseError:
                          raise: ${e}

# TODO(ghaun): Update so it doesn't create transfer configs on schedule. Currently makes it run ever 24 hours.
createDataExportIfNeeded:
  params: [projectId, orgId, datasetId]
  steps:
  - init:
      assign:
        - transferConfigId: null
  - checkIfTransferConfigExists:
      try:
        steps:
          - getAllConfigs:
              call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.list
              args:
                parent: ${"projects/" + projectId}
              result: getResults
          - findConfigId:
              for:
                value: transferConfig
                in: ${getResults.transferConfigs}
                steps:
                  - checkCondition:
                      switch:
                        - condition: ${transferConfig.displayName == "RecommenderAPIExport"}
                          steps:
                            - export_configId:
                                assign:
                                  - transferConfigId: ${transferConfig.name}
                                # returning as there is no need to continue looping.
                                next: break
                  - checkIfTransferConfigWasFound:
                      switch:
                        - condition: ${transferConfigId == null}
                          steps:
                            - raiseMissingError:
                                raise: "RecommenderAPIExport was not found in transfer configs"
                  - finalExists:
                      return: ${transferConfigId}
      except:
        as: e
        steps:
          # Add the transfer config 
          - createTransferConfig:
              call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.create
              args:
                parent: ${"projects/" + projectId}
                body:
                  "name": "RecommenderAPIExport"
                  "displayName": "RecommenderAPIExport"
                  "dataSourceId": "6063d10f-0000-2c12-a706-f403045e6250"
                  "params": 
                    "organization_id": ${orgId}
                  "destinationDatasetId": ${datasetId} 
              result: createResult
          - finalCreated:
              assign: 
                - transferConfigId: ${createResult.name}
  - finalReturn:
      return: ${transferConfigId}


runExportsInParallel:
  params: [bqDataset, bqTable, projectId, orgId, levelToExport, transferId]
  steps:
    - runningExports:
        parallel:
          shared: [bqDataset, bqTable, projectId]
          branches:
            # To make this call you need this API enabled: https://console.cloud.google.com/marketplace/product/google/cloudasset.googleapis.com?q=search&referrer=search&project=automating-container-rebuild
            # Service account needs cloud asset viewer. Probably best if you create a single service account for this. 
            - assetInventoryBranch:
                steps:
                  - exportAssetInventory:
                    # Need to see if there is a workflows connector for this instead. 
                      call: http.post
                      args:
                        url: ${"https://cloudasset.googleapis.com/v1/"+ levelToExport +":exportAssets"}
                        body: 
                          outputConfig: 
                            bigqueryDestination: 
                              dataset: ${"projects/" + projectId + "/datasets/" + bqDataset}
                              table: ${bqTable}
                              # This will overwrite a table if data exists. 
                              force: true
                              #partictionSpec: object: https://cloud.google.com/asset-inventory/docs/reference/rest/v1/TopLevel/exportAssets#partitionspec
                              #separateTablesPerAssetType:true  https://cloud.google.com/asset-inventory/docs/reference/rest/v1/TopLevel/exportAssets#BigQueryDestination
                        auth:
                          type: OAuth2
                          scopes: "https://www.googleapis.com/auth/cloud-platform"
                      result: response
            - recommenderApiBranch:
                steps:
                  - exportRecomendations:
                    #Note Org and Folder level exports is not GA and is in private preview. For now will focus on ORG
                    # One interesting thing to note is you have to use a BQ transfer service. This is gonna get interesting.
                    # One down side to this method is the fact that you can only export at the Org level, which is gonna cause me a few issues. 
                      call: googleapis.bigquerydatatransfer.v1.projects.transferConfigs.startManualRuns
                      args:
                        parent: ${transferId}
                        body:
                          requested_run_time: ${time.format(sys.now())}